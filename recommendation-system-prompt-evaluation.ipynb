{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42a9e51a-9dad-408c-a90c-c8ef22123d4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Evaluating LLM Prompts for Recommendation Systems\n",
    "\n",
    "This notebook provides a comprehensive framework for testing and evaluating different prompts for LLM-based recommendation engines on Databricks. By the end of this notebook, you'll be able to:\n",
    "\n",
    "1. Define and test multiple prompt variants\n",
    "2. Evaluate prompts using several recommendation quality metrics\n",
    "3. Log performance to MLflow for easy comparison\n",
    "4. Visualize results to identify the best-performing prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1041ba1e-659f-46a6-8077-41a8c17fd301",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Setup\n",
    "First, let's install the necessary libraries and set up our environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "caaf95d8-7b66-4dee-a4a6-d05f0daaa866",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install mlflow pandas scikit-learn sentence-transformers openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "336e68f0-c27b-401f-a3cd-2a5586647947",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "host_name = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiUrl().get()\n",
    "os.environ[\"OPENAI_BASE_URL\"] = host_name  + \"/serving-endpoints/\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50ec38cb-458f-4b67-9c82-6d90213e51f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "current_username = dbutils.notebook.entry_point.getDbutils().notebook().getContext().userName().get()\n",
    "current_username"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfdbfdcf-00ab-40d3-aaf1-d529d6ee4462",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import mlflow\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# from databricks.sdk import WorkspaceClient\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import os\n",
    "from typing import List, Dict, Tuple, Any\n",
    "\n",
    "# Configure MLflow experiment\n",
    "mlflow.set_experiment(f\"/Users/{current_username}/LLM-Recommendation-Prompts\")\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "# Initialize sentence transformer for semantic similarity calculations\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61f7d9c7-2d3a-4760-b377-d714105b8bb5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Sample Data\n",
    "\n",
    "Let's create some sample user data and a catalog of items that we can use for our recommendation tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "248b28cb-645f-4a42-9d3d-9fef89ed57c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Sample users with their viewing/purchase history\n",
    "users = [\n",
    "    {\n",
    "        \"user_id\": \"user_001\",\n",
    "        \"history\": \"The user has watched Inception, The Matrix, Interstellar, and Arrival. They also liked Dark on Netflix.\",\n",
    "        \"preferred_genres\": [\"sci-fi\", \"thriller\", \"mystery\"],\n",
    "        \"average_rating\": 4.5\n",
    "    },\n",
    "    {\n",
    "        \"user_id\": \"user_002\",\n",
    "        \"history\": \"The user enjoys comedies like The Hangover, Bridesmaids, and Superbad. They also watched The Office and Brooklyn Nine-Nine.\",\n",
    "        \"preferred_genres\": [\"comedy\", \"sitcom\"],\n",
    "        \"average_rating\": 4.2\n",
    "    },\n",
    "    {\n",
    "        \"user_id\": \"user_003\",\n",
    "        \"history\": \"The user has watched documentaries including Planet Earth, Making a Murderer, and Tiger King. They also enjoyed Mindhunter.\",\n",
    "        \"preferred_genres\": [\"documentary\", \"true crime\"],\n",
    "        \"average_rating\": 4.7\n",
    "    },\n",
    "    {\n",
    "        \"user_id\": \"user_004\",\n",
    "        \"history\": \"The user likes action movies such as John Wick, The Bourne Identity, and Mission Impossible. They also watched Extraction and The Old Guard.\",\n",
    "        \"preferred_genres\": [\"action\", \"thriller\"],\n",
    "        \"average_rating\": 4.0\n",
    "    }\n",
    "]\n",
    "\n",
    "# Sample catalog of items that could be recommended\n",
    "catalog = [\n",
    "    {\"item_id\": \"movie_001\", \"title\": \"Blade Runner 2049\", \"genres\": [\"sci-fi\", \"thriller\", \"mystery\"], \"description\": \"A young blade runner's discovery of a long-buried secret leads him to track down former blade runner Rick Deckard, who's been missing for thirty years.\"},\n",
    "    {\"item_id\": \"movie_002\", \"title\": \"Gravity\", \"genres\": [\"sci-fi\", \"thriller\", \"drama\"], \"description\": \"Two astronauts work together to survive after an accident leaves them stranded in space.\"},\n",
    "    {\"item_id\": \"movie_003\", \"title\": \"The Martian\", \"genres\": [\"sci-fi\", \"adventure\", \"drama\"], \"description\": \"An astronaut becomes stranded on Mars after his team assume him dead, and must rely on his ingenuity to find a way to signal to Earth that he is alive.\"},\n",
    "    {\"item_id\": \"movie_004\", \"title\": \"Knives Out\", \"genres\": [\"mystery\", \"comedy\", \"crime\"], \"description\": \"A detective investigates the death of a patriarch of an eccentric, combative family.\"},\n",
    "    {\"item_id\": \"movie_005\", \"title\": \"Palm Springs\", \"genres\": [\"comedy\", \"romance\", \"fantasy\"], \"description\": \"Stuck in a time loop, two wedding guests develop a budding romance while living the same day over and over again.\"},\n",
    "    {\"item_id\": \"movie_006\", \"title\": \"The Social Dilemma\", \"genres\": [\"documentary\", \"drama\"], \"description\": \"Explores the dangerous human impact of social networking, with tech experts sounding the alarm on their own creations.\"},\n",
    "    {\"item_id\": \"movie_007\", \"title\": \"My Octopus Teacher\", \"genres\": [\"documentary\", \"nature\"], \"description\": \"A filmmaker forges an unusual friendship with an octopus living in a South African kelp forest, learning as the animal shares the mysteries of her world.\"},\n",
    "    {\"item_id\": \"movie_008\", \"title\": \"The Gentlemen\", \"genres\": [\"action\", \"crime\", \"comedy\"], \"description\": \"An American expat tries to sell off his highly profitable marijuana empire in London, triggering plots, schemes, bribery and blackmail in an attempt to steal his domain out from under him.\"},\n",
    "    {\"item_id\": \"movie_009\", \"title\": \"Tenet\", \"genres\": [\"sci-fi\", \"action\", \"thriller\"], \"description\": \"Armed with only one word, Tenet, and fighting for the survival of the entire world, a Protagonist journeys through a twilight world of international espionage on a mission that will unfold in something beyond real time.\"},\n",
    "    {\"item_id\": \"movie_010\", \"title\": \"Parasite\", \"genres\": [\"thriller\", \"drama\", \"comedy\"], \"description\": \"Greed and class discrimination threaten the newly formed symbiotic relationship between the wealthy Park family and the destitute Kim clan.\"}\n",
    "]\n",
    "\n",
    "# Convert to DataFrames for easier handling\n",
    "users_df = pd.DataFrame(users)\n",
    "catalog_df = pd.DataFrame(catalog)\n",
    "\n",
    "# Display sample data\n",
    "display(users_df)\n",
    "display(catalog_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "edd6d858-f7d6-4d0f-879b-e207b4503e6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Define Prompt Templates\n",
    "\n",
    "Now, let's define different prompt templates that we'll evaluate. Each template represents a different approach to making recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "960fce3a-6c6a-40ba-8528-2e91077f63a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define various prompt templates for recommendation\n",
    "prompt_templates = {\n",
    "    \"basic\": \"\"\"\n",
    "    You are a recommendation system. Based on the user's history:\n",
    "    {user_history}\n",
    "    \n",
    "    Please recommend 5 items from the following catalog that the user would likely enjoy:\n",
    "    {catalog_items}\n",
    "    \n",
    "    Format your response as a comma-separated list of titles.\n",
    "    \"\"\",\n",
    "    \n",
    "    \"detailed\": \"\"\"\n",
    "    You are a recommendation system tasked with suggesting relevant items to users based on their viewing history.\n",
    "    \n",
    "    USER HISTORY:\n",
    "    {user_history}\n",
    "    \n",
    "    PREFERRED GENRES: {preferred_genres}\n",
    "    \n",
    "    CATALOG:\n",
    "    {catalog_items}\n",
    "    \n",
    "    Please analyze the user's viewing history and preferred genres, then recommend 5 items from the catalog that they would likely enjoy.\n",
    "    For each recommendation, provide a brief explanation of why it matches the user's preferences.\n",
    "    Format your response as a JSON list with 'title' and 'reason' keys.\n",
    "    \"\"\",\n",
    "    \n",
    "    \"structured_output\": \"\"\"\n",
    "    You are an AI recommendation engine. Your task is to analyze user preferences and recommend items from a catalog.\n",
    "    \n",
    "    USER DATA:\n",
    "    - Viewing History: {user_history}\n",
    "    - Preferred Genres: {preferred_genres}\n",
    "    - Average Rating: {avg_rating}\n",
    "    \n",
    "    AVAILABLE ITEMS:\n",
    "    {catalog_items}\n",
    "    \n",
    "    INSTRUCTIONS:\n",
    "    1. Analyze the user's viewing history and preferences\n",
    "    2. Identify patterns in their content consumption\n",
    "    3. Select 5 items from the catalog that best match their taste\n",
    "    4. Rank them by relevance score (0-100)\n",
    "    \n",
    "    Return your recommendations in the following JSON format:\n",
    "    {{\n",
    "        \"recommendations\": [\n",
    "            {{\"title\": \"Item Title\", \"relevance_score\": 85, \"reasoning\": \"Brief explanation\"}},\n",
    "            ...\n",
    "        ]\n",
    "    }}\n",
    "    \"\"\",\n",
    "    \n",
    "    \"contextual\": \"\"\"\n",
    "    You are a personalized recommendation system for a streaming platform. The user is currently browsing in the evening on a weekend and has just finished watching {last_watched}.\n",
    "    \n",
    "    USER PROFILE:\n",
    "    {user_history}\n",
    "    Genres they enjoy: {preferred_genres}\n",
    "    \n",
    "    AVAILABLE CONTENT:\n",
    "    {catalog_items}\n",
    "    \n",
    "    Based on what they've just watched and their overall preferences, recommend 5 titles they might want to watch next. Consider both similarity to their just-finished content and their general taste profile.\n",
    "    \n",
    "    Format your response as a JSON list with 'title', 'relevance_score' (0-100), and 'watch_next_reason' keys.\n",
    "    \"\"\",\n",
    "    \n",
    "    \"few_shot\": \"\"\"\n",
    "    You are a recommendation system. Based on viewing patterns, recommend items from a catalog.\n",
    "    \n",
    "    EXAMPLES:\n",
    "    \n",
    "    User History: \"The user enjoyed The Lord of the Rings trilogy and The Hobbit. They also liked Game of Thrones.\"\n",
    "    Catalog: [\"Harry Potter series\", \"The Witcher\", \"Dune\", \"Star Wars\", \"The Crown\"]\n",
    "    Recommendations: \n",
    "    1. The Witcher - Fantasy series with epic world-building similar to Game of Thrones\n",
    "    2. Dune - Epic sci-fi with complex world similar to Lord of the Rings\n",
    "    \n",
    "    User History: \"The user enjoys cooking shows like Chef's Table and Salt Fat Acid Heat.\"\n",
    "    Catalog: [\"The Great British Bake Off\", \"Anthony Bourdain: Parts Unknown\", \"Queer Eye\", \"Formula 1: Drive to Survive\"]\n",
    "    Recommendations:\n",
    "    1. Anthony Bourdain: Parts Unknown - Food-focused documentary series\n",
    "    2. The Great British Bake Off - Competition cooking show\n",
    "    \n",
    "    NOW YOUR TASK:\n",
    "    \n",
    "    User History: {user_history}\n",
    "    Preferred Genres: {preferred_genres}\n",
    "    Catalog: {catalog_items}\n",
    "    \n",
    "    Provide 5 recommendations with brief explanations of why they match the user's taste.\n",
    "    \"\"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f8f93e3-0abf-422d-934d-745467ea667e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Evaluation Metrics\n",
    "\n",
    "Let's define functions to evaluate the quality of recommendations:\n",
    "\n",
    "1. **Response Time**: How long it takes to generate recommendations\n",
    "2. **Relevance Score**: How well recommendations match user preferences\n",
    "3. **Diversity Score**: How varied the recommendations are\n",
    "4. **Consistency**: Whether the model gives similar recommendations for similar inputs\n",
    "5. **Formatting Compliance**: Whether the output follows the requested format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b943de8e-7c2d-4a40-a7d5-8b16fc3d2b43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def prepare_catalog_text(catalog_items: List[Dict]) -> str:\n",
    "    \"\"\"Convert catalog items to a text representation for the prompt\"\"\"\n",
    "    catalog_text = \"\"\n",
    "    for item in catalog_items:\n",
    "        catalog_text += f\"- {item['title']}: {item['description']} [Genres: {', '.join(item['genres'])}]\\n\"\n",
    "    return catalog_text\n",
    "\n",
    "def calculate_semantic_similarity(text1: str, text2: str) -> float:\n",
    "    \"\"\"Calculate semantic similarity between two texts using sentence embeddings\"\"\"\n",
    "    embedding1 = embedding_model.encode([text1])[0]\n",
    "    embedding2 = embedding_model.encode([text2])[0]\n",
    "    \n",
    "    # Calculate cosine similarity\n",
    "    similarity = cosine_similarity([embedding1], [embedding2])[0][0]\n",
    "    return similarity\n",
    "\n",
    "def calculate_diversity_score(recommendations: List[str], catalog_items: List[Dict]) -> float:\n",
    "    \"\"\"Calculate diversity score based on genre variety in recommendations\"\"\"\n",
    "    # Extract recommended items from catalog\n",
    "    recommended_items = [item for item in catalog_items if item['title'] in recommendations]\n",
    "    \n",
    "    # Get all genres from recommendations\n",
    "    all_genres = []\n",
    "    for item in recommended_items:\n",
    "        all_genres.extend(item['genres'])\n",
    "    \n",
    "    # Count unique genres\n",
    "    unique_genres = set(all_genres)\n",
    "    \n",
    "    # Calculate diversity score (normalized by total possible genres)\n",
    "    all_possible_genres = set()\n",
    "    for item in catalog_items:\n",
    "        all_possible_genres.update(item['genres'])\n",
    "    \n",
    "    diversity_score = len(unique_genres) / len(all_possible_genres) if all_possible_genres else 0\n",
    "    \n",
    "    return diversity_score\n",
    "\n",
    "def parse_recommendations(response: str, output_format: str) -> List[str]:\n",
    "    \"\"\"Extract recommended titles from LLM response based on expected format\"\"\"\n",
    "    if output_format == \"comma_list\":\n",
    "        # Parse comma-separated list\n",
    "        return [title.strip() for title in response.split(',')]\n",
    "    \n",
    "    elif output_format in [\"json\", \"structured\"]:\n",
    "        try:\n",
    "            # Try to parse JSON response\n",
    "            data = json.loads(response)\n",
    "            if \"recommendations\" in data:\n",
    "                return [rec[\"title\"] for rec in data[\"recommendations\"]]\n",
    "            else:\n",
    "                return [rec[\"title\"] for rec in data]\n",
    "        except json.JSONDecodeError:\n",
    "            # If JSON parsing fails, try to extract titles using simpler method\n",
    "            titles = []\n",
    "            for line in response.split('\\n'):\n",
    "                if '\"title\":' in line:\n",
    "                    title = line.split('\"title\":')[1].split('\"')[1]\n",
    "                    titles.append(title)\n",
    "            return titles\n",
    "    \n",
    "    else:\n",
    "        # Default fallback - look for numbered list or items in quotes\n",
    "        titles = []\n",
    "        for line in response.split('\\n'):\n",
    "            # Look for numbered items\n",
    "            if line.strip().startswith(('1.', '2.', '3.', '4.', '5.')):\n",
    "                parts = line.split('-', 1)[0].strip()\n",
    "                # Remove number and potential period\n",
    "                title = parts.split('.', 1)[1].strip() if '.' in parts else parts\n",
    "                titles.append(title)\n",
    "                \n",
    "            # Look for titles in quotes\n",
    "            elif '\"' in line and ':' in line:\n",
    "                potential_title = line.split('\"')[1]\n",
    "                if potential_title:\n",
    "                    titles.append(potential_title)\n",
    "        \n",
    "        return titles if titles else [line.strip() for line in response.split('\\n') if line.strip()]\n",
    "\n",
    "def check_format_compliance(response: str, expected_format: str) -> float:\n",
    "    \"\"\"Check if the response follows the requested format\"\"\"\n",
    "    if expected_format == \"comma_list\":\n",
    "        # Check if response is a comma-separated list\n",
    "        items = response.split(',')\n",
    "        return 1.0 if len(items) > 1 else 0.0\n",
    "    \n",
    "    elif expected_format == \"json\":\n",
    "        # Check if response is valid JSON\n",
    "        try:\n",
    "            json.loads(response)\n",
    "            return 1.0\n",
    "        except json.JSONDecodeError:\n",
    "            return 0.0\n",
    "    \n",
    "    elif expected_format == \"numbered_list\":\n",
    "        # Check if response contains numbered items\n",
    "        has_numbers = any(line.strip().startswith(('1.', '2.', '3.')) for line in response.split('\\n'))\n",
    "        return 1.0 if has_numbers else 0.0\n",
    "    \n",
    "    # Default case\n",
    "    return 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "be225d33-d0f6-4c11-a542-67815ee7d151",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## LLM Recommendation Function\n",
    "\n",
    "Now, let's create a function to generate recommendations using different LLM providers and prompt templates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba7a7d21-0f77-423c-9dc1-833d8f2cbfef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_recommendations(\n",
    "    user: Dict,\n",
    "    catalog_items: List[Dict],\n",
    "    prompt_template: str,\n",
    "    model_source: str = \"databricks-claude-3-5-sonnet\",\n",
    "    model_name: str = None,\n",
    "    output_format: str = \"comma_list\",\n",
    "    max_tokens: int = 500,\n",
    "    temperature: float = 0.7\n",
    ") -> Tuple[str, List[str], float]:\n",
    "    \"\"\"\n",
    "    Generate recommendations using an LLM with the specified prompt template\n",
    "    \n",
    "    Args:\n",
    "        user: User information including history and preferences\n",
    "        catalog_items: List of items to recommend from\n",
    "        prompt_template: Template string for the prompt\n",
    "        model_source: 'databricks-claude-3-5-sonnet', any model deployed on PT or external model\n",
    "        model_name: Specific model to use\n",
    "        output_format: Expected format of the response\n",
    "        max_tokens: Maximum tokens in response\n",
    "        temperature: Temperature parameter for generation\n",
    "        \n",
    "    Returns:\n",
    "        Tuple containing (raw_response, parsed_recommendations, response_time)\n",
    "    \"\"\"\n",
    "    # Start timer\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Get the most recently watched item for contextual prompts\n",
    "    last_watched = \"unknown\"\n",
    "    if user[\"history\"]:\n",
    "        history_text = user[\"history\"]\n",
    "        if \"watched\" in history_text or \"liked\" in history_text:\n",
    "            # Simple extraction of last mentioned title - this could be improved\n",
    "            all_items = history_text.replace(\".\", \"\").replace(\",\", \"\").split()\n",
    "            last_watched = all_items[-1] if all_items else \"unknown\"\n",
    "    \n",
    "    # Format catalog items as text\n",
    "    catalog_text = prepare_catalog_text(catalog_items)\n",
    "    \n",
    "    # Format preferred genres\n",
    "    preferred_genres_text = \", \".join(user[\"preferred_genres\"]) if \"preferred_genres\" in user else \"\"\n",
    "    \n",
    "    # Fill in the prompt template\n",
    "    input_prompt = prompt_template.format(\n",
    "        user_history=user[\"history\"],\n",
    "        catalog_items=catalog_text,\n",
    "        preferred_genres=preferred_genres_text,\n",
    "        avg_rating=user.get(\"average_rating\", \"\"),\n",
    "        last_watched=last_watched\n",
    "    )\n",
    "\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": input_prompt\n",
    "            }\n",
    "            ],\n",
    "            model=model_source,\n",
    "            max_tokens=256\n",
    "        )\n",
    "\n",
    "    raw_response = chat_completion.choices[0].message.content.strip()\n",
    "    \n",
    "    # Calculate response time\n",
    "    response_time = time.time() - start_time\n",
    "    \n",
    "    # Parse the recommendations from the response\n",
    "    recommendations = parse_recommendations(raw_response, output_format)\n",
    "    \n",
    "    return raw_response, recommendations, response_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fb245ffb-7434-48d5-b002-c3c13240048f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Evaluate and Log Prompt Performance\n",
    "\n",
    "Now, let's create a function to evaluate a prompt and log its performance metrics to MLflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49ab6aa9-0a38-47d3-850b-388726247a94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_prompt_performance(\n",
    "    prompt_name: str,\n",
    "    prompt_template: str,\n",
    "    user: Dict,\n",
    "    catalog_items: List[Dict],\n",
    "    model_source: str = \"databricks-claude-3-5-sonnet\",\n",
    "    model_name: str = None,\n",
    "    output_format: str = \"comma_list\",\n",
    "    run_name: str = None\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Evaluate a prompt's performance and log metrics to MLflow\n",
    "    \n",
    "    Args:\n",
    "        prompt_name: Name of the prompt for logging\n",
    "        prompt_template: The prompt template to evaluate\n",
    "        user: User information\n",
    "        catalog_items: Catalog items to recommend from\n",
    "        model_source: LLM provider, any model deployed on PT or external model\n",
    "        model_name: Specific model name\n",
    "        output_format: Expected output format\n",
    "        run_name: Optional name for the MLflow run\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary of metrics\n",
    "    \"\"\"\n",
    "    # Generate a run name if not provided\n",
    "    if run_name is None:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        run_name = f\"{prompt_name}-{model_source}-{timestamp}\"\n",
    "    \n",
    "    # Start MLflow run\n",
    "    with mlflow.start_run(run_name=run_name) as run:\n",
    "        # Log parameters\n",
    "        mlflow.log_param(\"prompt_name\", prompt_name)\n",
    "        mlflow.log_param(\"model_source\", model_source)\n",
    "        mlflow.log_param(\"model_name\", model_name)\n",
    "        mlflow.log_param(\"user_id\", user[\"user_id\"])\n",
    "        mlflow.log_param(\"output_format\", output_format)\n",
    "        \n",
    "        # Log the actual prompt as an artifact\n",
    "        with open(\"prompt.txt\", \"w\") as f:\n",
    "            f.write(prompt_template.format(\n",
    "                user_history=user[\"history\"],\n",
    "                catalog_items=prepare_catalog_text(catalog_items),\n",
    "                preferred_genres=\", \".join(user[\"preferred_genres\"]) if \"preferred_genres\" in user else \"\",\n",
    "                avg_rating=user.get(\"average_rating\", \"\"),\n",
    "                last_watched=\"unknown\"  # Simplified for logging\n",
    "            ))\n",
    "        mlflow.log_artifact(\"prompt.txt\")\n",
    "        \n",
    "        # Generate recommendations\n",
    "        raw_response, recommendations, response_time = get_recommendations(\n",
    "            user=user,\n",
    "            catalog_items=catalog_items,\n",
    "            prompt_template=prompt_template,\n",
    "            model_source=model_source,\n",
    "            model_name=model_name,\n",
    "            output_format=output_format\n",
    "        )\n",
    "        \n",
    "        # Log the raw response\n",
    "        with open(\"response.txt\", \"w\") as f:\n",
    "            f.write(raw_response)\n",
    "        mlflow.log_artifact(\"response.txt\")\n",
    "        \n",
    "        # Calculate metrics\n",
    "        \n",
    "        # 1. Response Time\n",
    "        mlflow.log_metric(\"response_time\", response_time)\n",
    "        \n",
    "        # 2. Relevance Score - based on similarity between user history and recommendations\n",
    "        # Extract recommendation descriptions\n",
    "        recommended_items = [item for item in catalog_items if item['title'] in recommendations]\n",
    "        recommendation_text = \" \".join([item[\"description\"] for item in recommended_items])\n",
    "        relevance_score = calculate_semantic_similarity(user[\"history\"], recommendation_text)\n",
    "        mlflow.log_metric(\"relevance_score\", relevance_score)\n",
    "        \n",
    "        # 3. Genre Match Score - how well recommendations match preferred genres\n",
    "        if \"preferred_genres\" in user and user[\"preferred_genres\"]:\n",
    "            genre_match_count = 0\n",
    "            total_genres = 0\n",
    "            \n",
    "            for item in recommended_items:\n",
    "                matching_genres = set(item[\"genres\"]) & set(user[\"preferred_genres\"])\n",
    "                genre_match_count += len(matching_genres)\n",
    "                total_genres += len(item[\"genres\"])\n",
    "            \n",
    "            genre_match_score = genre_match_count / total_genres if total_genres > 0 else 0\n",
    "            mlflow.log_metric(\"genre_match_score\", genre_match_score)\n",
    "        \n",
    "        # 4. Diversity Score\n",
    "        diversity_score = calculate_diversity_score(recommendations, catalog_items)\n",
    "        mlflow.log_metric(\"diversity_score\", diversity_score)\n",
    "        \n",
    "        # 5. Format Compliance\n",
    "        format_compliance = check_format_compliance(raw_response, output_format)\n",
    "        mlflow.log_metric(\"format_compliance\", format_compliance)\n",
    "        \n",
    "        # 6. Number of recommendations\n",
    "        rec_count = len(recommendations)\n",
    "        mlflow.log_metric(\"recommendation_count\", rec_count)\n",
    "        \n",
    "        # Calculate composite score (weighted average of metrics)\n",
    "        composite_score = (\n",
    "            0.3 * relevance_score + \n",
    "            0.2 * diversity_score + \n",
    "            0.2 * (genre_match_score if \"preferred_genres\" in user else 0.5) + \n",
    "            0.2 * format_compliance + \n",
    "            0.1 * (1.0 - min(1.0, response_time / 10.0))  # Normalize response time\n",
    "        )\n",
    "        mlflow.log_metric(\"composite_score\", composite_score)\n",
    "        \n",
    "        # Prepare results dictionary\n",
    "        metrics = {\n",
    "            \"run_id\": run.info.run_id,\n",
    "            \"prompt_name\": prompt_name,\n",
    "            \"model_source\": model_source,\n",
    "            \"response_time\": response_time,\n",
    "            \"relevance_score\": relevance_score,\n",
    "            \"diversity_score\": diversity_score,\n",
    "            \"format_compliance\": format_compliance,\n",
    "            \"recommendation_count\": rec_count,\n",
    "            \"composite_score\": composite_score,\n",
    "            \"recommendations\": recommendations,\n",
    "            \"raw_response\": raw_response\n",
    "        }\n",
    "        \n",
    "        if \"preferred_genres\" in user:\n",
    "            metrics[\"genre_match_score\"] = genre_match_score\n",
    "            \n",
    "        print(f\"Evaluated prompt: {prompt_name}\")\n",
    "        print(f\"Composite Score: {composite_score:.4f}\")\n",
    "        print(f\"Recommendations: {', '.join(recommendations[:5])}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "35cab840-012d-405e-9fc9-a0ad493fa018",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Run Evaluations\n",
    "\n",
    "Now, let's evaluate all our prompts across different users and models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8106584-7a9b-4c32-8783-508dbd70ffe9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_all_prompts(\n",
    "    users: List[Dict],\n",
    "    catalog_items: List[Dict],\n",
    "    prompt_templates: Dict[str, str],\n",
    "    model_sources: List[str] = [\"databricks-claude-3-5-sonnet\", \"databricks-meta-llama-3-3-70b-instruct\"],\n",
    "    output_formats: Dict[str, str] = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Evaluate all prompt templates across users and models\n",
    "    \n",
    "    Args:\n",
    "        users: List of user information\n",
    "        catalog_items: List of catalog items\n",
    "        prompt_templates: Dictionary of prompt templates\n",
    "        model_sources: List of model sources to evaluate\n",
    "        output_formats: Dictionary mapping prompt names to expected output formats\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with evaluation results\n",
    "    \"\"\"\n",
    "    # Default output formats if not provided\n",
    "    if output_formats is None:\n",
    "        output_formats = {\n",
    "            \"basic\": \"comma_list\",\n",
    "            \"detailed\": \"json\",\n",
    "            \"structured_output\": \"json\",\n",
    "            \"contextual\": \"json\",\n",
    "            \"few_shot\": \"numbered_list\"\n",
    "        }\n",
    "    \n",
    "    # Store all results\n",
    "    all_results = []\n",
    "    \n",
    "    # Evaluate each prompt for each user and model source\n",
    "    for user in users:\n",
    "        for prompt_name, prompt_template in prompt_templates.items():\n",
    "            output_format = output_formats.get(prompt_name, \"comma_list\")\n",
    "            \n",
    "            for model_source in model_sources:\n",
    "                # Evaluate the prompt\n",
    "                result = evaluate_prompt_performance(\n",
    "                    prompt_name=prompt_name,\n",
    "                    prompt_template=prompt_template,\n",
    "                    user=user,\n",
    "                    catalog_items=catalog_items,\n",
    "                    model_source=model_source,\n",
    "                    output_format=output_format\n",
    "                )\n",
    "                \n",
    "                # Add user info to result\n",
    "                result[\"user_id\"] = user[\"user_id\"]\n",
    "                \n",
    "                # Append to results list\n",
    "                all_results.append(result)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18acbf82-fdfb-41e4-98f8-1e05d149ca7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Run evaluations for a subset of users and model sources\n",
    "# In a real scenario, you might want to run this for all users and models\n",
    "evaluation_results = evaluate_all_prompts(\n",
    "    users=users[:2],  # Use the first two users for this example\n",
    "    catalog_items=catalog,\n",
    "    prompt_templates=prompt_templates,\n",
    "    model_sources=[\"databricks-meta-llama-3-3-70b-instruct\"]  # Use Databricks LLM for this example\n",
    ")\n",
    "\n",
    "# Display results\n",
    "display(evaluation_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "52006b84-61f6-422f-b9bc-44ac54cb92b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Visualize Results\n",
    "\n",
    "Let's create visualizations to compare the performance of different prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6d14587-3acd-4dd7-80b2-ebd6c027594b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def plot_prompt_comparison(results_df: pd.DataFrame, metric: str = \"composite_score\"):\n",
    "    \"\"\"Create a bar plot comparing prompts based on a specific metric\"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Group by prompt_name and calculate the mean of the specified metric\n",
    "    prompt_performance = results_df.groupby('prompt_name')[metric].mean().reset_index()\n",
    "    \n",
    "    # Sort by the metric value\n",
    "    prompt_performance = prompt_performance.sort_values(by=metric, ascending=False)\n",
    "    \n",
    "    # Create the bar plot\n",
    "    sns.barplot(x='prompt_name', y=metric, data=prompt_performance)\n",
    "    \n",
    "    plt.title(f'Prompt Comparison by {metric.replace(\"_\", \" \").title()}')\n",
    "    plt.xlabel('Prompt Template')\n",
    "    plt.ylabel(metric.replace(\"_\", \" \").title())\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the figure for MLflow\n",
    "    plt.savefig(f\"{metric}_comparison.png\")\n",
    "    \n",
    "    return plt.gcf()\n",
    "\n",
    "def plot_metric_radar(results_df: pd.DataFrame):\n",
    "    \"\"\"Create a radar plot comparing prompts across multiple metrics\"\"\"\n",
    "    # Metrics to include in the radar plot\n",
    "    metrics = [\"relevance_score\", \"diversity_score\", \"format_compliance\", \n",
    "               \"genre_match_score\", \"recommendation_count\"]\n",
    "    \n",
    "    # Ensure all metrics exist in the DataFrame\n",
    "    metrics = [m for m in metrics if m in results_df.columns]\n",
    "    \n",
    "    # Group by prompt_name and calculate the mean of each metric\n",
    "    prompt_metrics = results_df.groupby('prompt_name')[metrics].mean()\n",
    "    \n",
    "    # Normalize each metric to [0, 1] for the radar plot\n",
    "    for metric in metrics:\n",
    "        max_val = prompt_metrics[metric].max()\n",
    "        if max_val > 0:\n",
    "            prompt_metrics[metric] = prompt_metrics[metric] / max_val\n",
    "    \n",
    "    # Number of variables\n",
    "    N = len(metrics)\n",
    "    \n",
    "    # What will be the angle of each axis in the plot\n",
    "    angles = [n / float(N) * 2 * np.pi for n in range(N)]\n",
    "    angles += angles[:1]  # Close the loop\n",
    "    \n",
    "    # Create the radar plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(polar=True))\n",
    "    \n",
    "    # Add labels for each metric\n",
    "    plt.xticks(angles[:-1], metrics, size=12)\n",
    "    \n",
    "    # Draw one axis per variable and add labels\n",
    "    # for i, metric in enumerate(metrics):\n",
    "    #     angle = angles[i]\n",
    "    #     ax.text(angle, 1.2, metric.replace(\"_\", \" \").title(), \n",
    "    #             horizontalalignment='center', size=14)\n",
    "    \n",
    "    # Plot each prompt\n",
    "    for prompt_name in prompt_metrics.index:\n",
    "        values = prompt_metrics.loc[prompt_name].values.tolist()\n",
    "        values += values[:1]  # Close the loop\n",
    "        \n",
    "        # Plot values\n",
    "        ax.plot(angles, values, linewidth=2, linestyle='solid', label=prompt_name)\n",
    "        ax.fill(angles, values, alpha=0.1)\n",
    "    \n",
    "    # Add legend\n",
    "    plt.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))\n",
    "    plt.title(\"Prompt Comparison Across Metrics\", size=20)\n",
    "    \n",
    "    # Save the figure for MLflow\n",
    "    plt.savefig(\"radar_comparison.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97088dd1-8693-4c98-bdb2-4303c6da15e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plot_metric_radar(evaluation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4acffd07-8126-44ac-9e12-d930840969be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plot_prompt_comparison(evaluation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "14c1052c-799f-4760-9452-a70fdff19cb9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "recommendation-system-prompt-evaluation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
