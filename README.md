
# ğŸš€ LLM-Based Recommendation System: Prompt Evaluation on Databricks

This repository provides a **Databricks notebook** for **evaluating and optimizing LLM prompt templates** in a recommendation system. It enables **data scientists and engineers** to experiment with multiple prompts, track performance metrics using **MLflow**, and visualize prompt effectiveness.

## ğŸ“Œ Features
- âœ… **Multi-Prompt Evaluation**: Test different LLM prompt styles for recommendations.
- âœ… **MLflow Logging & Experimentation**: Track performance across multiple prompt runs.
- âœ… **Key Metrics**: Analyze **response time, relevance, diversity, genre match, and format compliance**.
- âœ… **Supports Any LLM Model**: Works with **Databricks-hosted LLMs, OpenAI API, or custom endpoints**.
- âœ… **Visualization**: Compare prompt effectiveness using plots and MLflow UI.

---

## ğŸ“‚ Repository Structure
```
â”œâ”€â”€ recommendation-system-prompt-evaluation.ipynb  # Main Databricks notebook
â”œâ”€â”€ images/                                        # (Optional) Stores performance visualizations
â”‚   â”œâ”€â”€ prompt_comparison.png
â”‚   â”œâ”€â”€ radar_comparison.png
â”œâ”€â”€ README.md                                      # Documentation
```

---

## âš™ï¸ How to Use This Notebook on Databricks

### 1ï¸âƒ£ **Import Notebook to Databricks**
Instead of cloning this repo manually, you can import the notebook directly into Databricks:

1. Open **Databricks Workspace**.
2. Navigate to the **Workspace** tab.
3. Click on **`Import`** at the top-right.
4. Select **`File`**, then upload `recommendation-system-prompt-evaluation.ipynb`.
5. Attach the notebook to a cluster with **MLflow enabled**.

### 2ï¸âƒ£ **Install Dependencies**
The notebook requires **Databricks MLflow, OpenAI, Pandas, Scikit-learn, Sentence Transformers, and Matplotlib**.
If any package is missing, install them using:
```python
%pip install mlflow pandas scikit-learn sentence-transformers openai matplotlib seaborn
```

### 3ï¸âƒ£ **Run the Notebook**
- Execute all cells to evaluate different LLM prompt templates.
- MLflow will log results automatically.

---

## ğŸ¯ How It Works
### ğŸ— **Step 1: Define Multiple Prompt Templates**
The notebook includes **several prompt styles**, including:
- **Basic** (Simple Recommendations)
- **Structured Output** (JSON format)
- **Contextual** (Dynamic User Preferences)
- **Few-Shot Examples** (Pre-Trained Patterns)

### ğŸ“Š **Step 2: Generate Recommendations**
Each prompt is passed to an LLM (e.g., OpenAI GPT, Databricks Llama 3), and the response is logged.

### ğŸ“ˆ **Step 3: Evaluate Performance**
Metrics like **relevance score, diversity score, response time**, and **format compliance** are calculated.

### ğŸ” **Step 4: Log & Compare with MLflow**
All evaluations are **tracked in MLflow**, making it easy to compare performance across multiple prompt templates.

---

## ğŸ“Œ Example Prompt Evaluation
```python
evaluate_prompt_performance(
    prompt_name="structured_output",
    prompt_template=structured_prompt,
    user=user_sample,
    catalog_items=movie_catalog,
    model_source="databricks-meta-llama-3-3-70b-instruct"
)
```
MLflow will log:
âœ… **Response Time**  
âœ… **Relevance Score**  
âœ… **Diversity Score**  
âœ… **Format Compliance**  

---

## ğŸ“Š Visualization Examples
**Comparison of prompt performance using radar plots and bar charts:**
![Prompt Comparison](images/prompt_comparison.png)

---

## ğŸš€ Best Practices for LLM-Based Recommendation Engines
1. **Use embeddings for retrieval**: Combine **vector search + LLMs** for best results.
2. **Track and version prompts**: Store different prompt styles and compare effectiveness.
3. **Optimize for response time**: Reduce token size and pre-filter candidates before passing to LLM.
4. **Leverage MLflow**: Log experiments to compare **LLM models, prompt styles, and system performance**.

---

## ğŸ“œ License
This project is licensed under the **MIT License**.

---

## ğŸ¤ Contributing
Contributions are welcome! Feel free to:
- Open an issue ğŸ“Œ
- Submit a pull request ğŸš€
- Share feedback in discussions ğŸ’¡

---

## ğŸ“¬ Contact
For questions or suggestions, reach out via **[GitHub Issues](#)** or email at **debusinha2009@gmail.com**.
